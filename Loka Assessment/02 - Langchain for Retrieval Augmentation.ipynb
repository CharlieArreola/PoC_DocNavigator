{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain for Retrieval Augmentation\n",
    "\n",
    "## Create and Index\n",
    "We can set up our index to store our data. We begin by initializing our connection to Pinecone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "pinecone_api = \"xxx\"\n",
    "index_name = \"sagemaker-agent\"\n",
    "pinecone_region = \"us-east-1\"\n",
    "pinecone_host = \"xxx\"\n",
    "pine_cloud = \"aws\"\n",
    "pinecone_metric = \"cosine\"\n",
    "\n",
    "openai_api = \"sk-xxx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set up our index to store our Data.\n",
    "\n",
    "We begin by initializing our connection to Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=pinecone_api)\n",
    "spec = ServerlessSpec(cloud=pine_cloud, region=pinecone_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete index if it exists\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# we create a new index\n",
    "pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
    "        metric=pinecone_metric,\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "# wait for index to be initialized\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "# wait a moment for connection\n",
    "time.sleep(1)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our pre-embedded data. We will format the dataframe to be ready for upserting into Pinecone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(337, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>values</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-1</td>\n",
       "      <td>[-0.008473106659948826, 0.016663100570440292, ...</td>\n",
       "      <td>{'chunk': 1, 'source': 'amazon-sagemaker-toolk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-1</td>\n",
       "      <td>[-0.01526849064975977, 0.029873132705688477, 0...</td>\n",
       "      <td>{'chunk': 1, 'source': 'asff-resourcedetails-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                             values  \\\n",
       "0  1-1  [-0.008473106659948826, 0.016663100570440292, ...   \n",
       "1  2-1  [-0.01526849064975977, 0.029873132705688477, 0...   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'chunk': 1, 'source': 'amazon-sagemaker-toolk...  \n",
       "1  {'chunk': 1, 'source': 'asff-resourcedetails-a...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we load the the pinecone dataset we prepared from the previous notebook\n",
    "file_path = \"CuratedData\\sagemaker_documentation_embeddings.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(columns=['document_name', 'title'])\n",
    "\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>values</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-1</td>\n",
       "      <td>[-0.008473106659948826, 0.016663100570440292, ...</td>\n",
       "      <td>{'chunk': 1, 'source': 'amazon-sagemaker-toolk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-1</td>\n",
       "      <td>[-0.01526849064975977, 0.029873132705688477, 0...</td>\n",
       "      <td>{'chunk': 1, 'source': 'asff-resourcedetails-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                             values  \\\n",
       "0  1-1  [-0.008473106659948826, 0.016663100570440292, ...   \n",
       "1  2-1  [-0.01526849064975977, 0.029873132705688477, 0...   \n",
       "\n",
       "                                            metadata  \n",
       "0  {'chunk': 1, 'source': 'amazon-sagemaker-toolk...  \n",
       "1  {'chunk': 1, 'source': 'asff-resourcedetails-a...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure 'values' column is properly formatted\n",
    "df['values'] = df['values'].apply(lambda x: [float(i) for i in x.strip('[]').split(',')])\n",
    "\n",
    "# Ensure 'metadata' column is properly formatted as a dictionary\n",
    "df['metadata'] = df['metadata'].apply(lambda x: ast.literal_eval(x))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to upsert our data in batches. This is useful when we have a large dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted batch 1/7\n",
      "Upserted batch 2/7\n",
      "Upserted batch 3/7\n",
      "Upserted batch 4/7\n",
      "Upserted batch 5/7\n",
      "Upserted batch 6/7\n",
      "Upserted batch 7/7\n"
     ]
    }
   ],
   "source": [
    "# Function to batch upload data to Pinecone\n",
    "def upsert_data_to_pinecone(index, df, batch_size=50):\n",
    "    time.sleep(10)\n",
    "    records = df.to_dict('records')\n",
    "    total_records = len(records)\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = records[i:i + batch_size]\n",
    "        vectors = [\n",
    "            {\n",
    "                \"id\": str(record['id']),\n",
    "                \"values\": record['values'],\n",
    "                \"metadata\": record['metadata']\n",
    "            }\n",
    "            for record in batch\n",
    "        ]\n",
    "        index.upsert(vectors=vectors)\n",
    "        print(f\"Upserted batch {i//batch_size + 1}/{(total_records // batch_size) + 1}\")\n",
    "\n",
    "# Upsert the data to Pinecone\n",
    "upsert_data_to_pinecone(index, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 337}},\n",
       " 'total_vector_count': 337}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the data has been upserted\n",
    "time.sleep(1) \n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's explore different options to solve the problem of retrieval augmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st - RetrievalQAWithSourcesChain\n",
    "The `RetrievalQAWithSourcesChain` in LangChain is a specialized chain designed to handle retrieval-based question answering tasks while providing source attribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=openai_api\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carlo\\anaconda3\\envs\\sagemaker_agent\\lib\\site-packages\\langchain_community\\vectorstores\\pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\" # Here is the content of the document in our metadata\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=openai_api,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(k=3)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is SageMaker?',\n",
       " 'answer': 'Amazon SageMaker is a fully managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning models. It integrates with AWS Marketplace, allowing developers to charge other SageMaker users for the use of their algorithms and model packages. SageMaker provides an integrated Jupyter authoring notebook instance for easy access to data sources and allows for the association of Git repositories with notebook instances. Before creating algorithm and model package resources, they must be developed and packaged in Docker containers. SageMaker is a powerful tool for machine learning development and deployment.\\n',\n",
       " 'sources': 'examples-sagemaker.md, sagemaker-marketplace.md, integrating-sagemaker.md, sagemaker-marketplace-develop.md'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "question = \"What is SageMaker?\"\n",
    "qa_with_sources(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are all AWS regions where SageMaker is available?',\n",
       " 'answer': 'The AWS regions where SageMaker is available are:\\n\\n- US East (Ohio)\\n- US East (N. Virginia)\\n- US West (N. California)\\n- US West (Oregon)\\n- Africa (Cape Town)\\n- Asia Pacific (Hong Kong)\\n- Asia Pacific (Mumbai)\\n- Asia Pacific (Osaka)\\n- Asia Pacific (Seoul)\\n- Asia Pacific (Singapore)\\n- Asia Pacific (Sydney)\\n- Asia Pacific (Jakarta)\\n- Asia Pacific (Tokyo)\\n- Canada (Central)\\n- China (Beijing)\\n- China (Ningxia)\\n- Europe (Frankfurt)\\n- Europe (Ireland)\\n- Europe (London)\\n- Europe (Paris)\\n- Europe (Stockholm)\\n- Europe (Milan)\\n- Middle East (Bahrain)\\n- South America (São Paulo)\\n- AWS GovCloud (US-West)\\n\\n',\n",
       " 'sources': 'sagemaker-algo-docker-registry-paths.md'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "question = \"What are all AWS regions where SageMaker is available?\"\n",
    "qa_with_sources(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How to check if an endpoint is KMS encrypted?',\n",
       " 'answer': 'To check if an endpoint is KMS encrypted, you need to verify whether the AWS Key Management Service (KMS) key is configured for an Amazon SageMaker endpoint configuration. The endpoint configuration is considered NON_COMPLIANT if \"KmsKeyId\" is not specified for the Amazon SageMaker endpoint configuration.\\n\\n',\n",
       " 'sources': 'sagemaker-endpoint-configuration-kms-key-configured.md'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "question = \"How to check if an endpoint is KMS encrypted?\"\n",
    "qa_with_sources(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are SageMaker Geospatial capabilities?',\n",
       " 'answer': 'Amazon SageMaker geospatial capabilities allow users to perform operations on AWS hardware managed by SageMaker, with the ability to create and use execution roles for specific permissions. These capabilities include actions like passing roles between services and attaching trust policies to IAM roles. Specific permissions are required for different API calls, such as StartEarthObservationJob and StartVectorEnrichmentJob. Users can also utilize AWS managed policies like AmazonSageMakerFullAccess for broader permissions. SageMaker also integrates with AWS Marketplace for selling algorithms and model packages. \\n',\n",
       " 'sources': 'sagemaker-geospatial-roles.md, integrating-sagemaker.md, sagemaker-marketplace.md'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "question = \"What are SageMaker Geospatial capabilities?\"\n",
    "qa_with_sources(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd - Retrieval Augmented Generation using ChatCompletions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=openai_api)\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "\n",
    "def get_embedding(text, model=embedding_model):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def rag_chatcompletions(query):\n",
    "    # Get the embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    # Retrieve from Pinecone\n",
    "    res = index.query(vector=query_embedding, top_k=5, include_metadata=True)\n",
    "\n",
    "    # Get list of retrieved text\n",
    "    contexts = [item['metadata']['text'] for item in res['matches']]\n",
    "\n",
    "    # Create the augmented query\n",
    "    augmented_query = \"\\n\\n---\\n\\n\".join(contexts) + \"\\n\\n-----\\n\\n\" + query\n",
    "    # System message to 'prime' the model\n",
    "    primer = \"\"\"You are Q&A bot. A highly intelligent system that answers\n",
    "    user questions based on the information provided by the user above\n",
    "    each question. If the information can not be found in the information\n",
    "    provided by the user you truthfully say \"I don't know\".\n",
    "    Remember to share the source of the content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the chat completion\n",
    "    response = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': primer},\n",
    "                {'role': 'user', 'content': augmented_query},\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "    display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "SageMaker is a fully managed machine learning service provided by Amazon. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, eliminating the need to manage servers. SageMaker also integrates with AWS Marketplace, allowing developers to charge other SageMaker users for the use of their algorithms and model packages."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the function\n",
    "question = \"What is SageMaker?\"\n",
    "rag_chatcompletions(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the information provided, SageMaker is available in the following AWS regions:\n",
       "\n",
       "1. US East (Ohio)\n",
       "2. US East (N. Virginia)\n",
       "3. US West (N. California)\n",
       "4. US West (Oregon)\n",
       "5. Africa (Cape Town)\n",
       "6. Asia Pacific (Hong Kong)\n",
       "7. Asia Pacific (Mumbai)\n",
       "8. Asia Pacific (Osaka)\n",
       "9. Asia Pacific (Seoul)\n",
       "10. Asia Pacific (Singapore)\n",
       "11. Asia Pacific (Sydney)\n",
       "12. Asia Pacific (Jakarta)\n",
       "13. Asia Pacific (Tokyo)\n",
       "14. Canada (Central)\n",
       "15. China (Beijing)\n",
       "16. China (Ningxia)\n",
       "17. Europe (Frankfurt)\n",
       "18. Europe (Ireland)\n",
       "19. Europe (London)\n",
       "20. Europe (Paris)\n",
       "21. Europe (Stockholm)\n",
       "22. Europe (Milan)\n",
       "23. Middle East (Bahrain)\n",
       "24. South America (São Paulo)\n",
       "25. AWS GovCloud (US-West)\n",
       "\n",
       "Please note that this list is based on the information provided and may not include all regions where SageMaker is available. For the most up-to-date list, please refer to the official AWS documentation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the function\n",
    "question = \"What are all AWS regions where SageMaker is available?\"\n",
    "rag_chatcompletions(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To check if an Amazon SageMaker endpoint is KMS encrypted, you can use the AWS Config managed rule with the identifier SAGEMAKER_ENDPOINT_CONFIGURATION_KMS_KEY_CONFIGURED. This rule checks whether an AWS Key Management Service (KMS) key is configured for an Amazon SageMaker endpoint configuration. The rule is NON_COMPLIANT if \"KmsKeyId\" is not specified for the Amazon SageMaker endpoint configuration. This rule is triggered periodically and applies to all supported AWS regions except a few specified ones."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the function\n",
    "question = \"How to check if an endpoint is KMS encrypted?\"\n",
    "rag_chatcompletions(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Amazon SageMaker Geospatial capabilities are part of the managed service provided by Amazon SageMaker. These capabilities allow users to perform operations on the AWS hardware that is managed by SageMaker. The operations can only be performed if the user grants permissions through an IAM role, also known as an execution role. This role grants the service permission to access the user's AWS resources. The geospatial capabilities are particularly useful for tasks that involve geographical data or mapping services."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the function\n",
    "question = \"What are SageMaker Geospatial capabilities?\"\n",
    "rag_chatcompletions(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "1. **Effectiveness of Retrieval Augmentation:**\n",
    "   The proof of concept (POC) demonstrated that integrating LangChain with Pinecone for retrieval augmentation can significantly reduce the time developers spend searching through documentation. This system effectively retrieves relevant information, addressing the primary challenge faced by Company X.\n",
    "\n",
    "2. **Scalability and Ease of Application:**\n",
    "   The first approach, utilizing Pinecone for vector indexing, proved to be more scalable and easier to implement. Its integration with LangChain allows for seamless expansion as additional documentation and data sources are incorporated. This approach is particularly beneficial for handling large volumes of data, ensuring the system remains efficient and responsive.\n",
    "\n",
    "3. **Compliance with Company Requirements:**\n",
    "   This approach better complies with the challenge provided by Company X. It ensures that the documentation system assists developers with unfamiliar parts of the documentation. Additionally, as mentioned in the \"nice to have,\" it provides the source document in the output, helping users verify and explore further.\n",
    "   \n",
    "   **Geographical restrictions:** Using Pinecone within the US region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
